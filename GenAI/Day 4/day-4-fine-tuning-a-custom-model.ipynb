{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"day-4-fine-tuning-a-custom-model.ipynb","toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2025 Google LLC.","metadata":{"id":"b6e13eef3f5d"}},{"cell_type":"code","source":"# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"cellView":"form","id":"d6597b11df14","jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:39:24.010656Z","iopub.execute_input":"2025-02-28T07:39:24.011577Z","iopub.status.idle":"2025-02-28T07:39:24.035703Z","shell.execute_reply.started":"2025-02-28T07:39:24.011515Z","shell.execute_reply":"2025-02-28T07:39:24.034263Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Day 4 - Fine tuning a custom model\n\nWelcome back to the Kaggle 5-day Generative AI course!\n\nIn this notebook you will use the Gemini API to fine-tune a custom, task-specific model. Fine-tuning can be used for a variety of tasks from classic NLP problems like entity extraction or summarisation, to creative tasks like stylised generation. You will fine-tune a model to classify the category a piece of text (a newsgroup post) into the category it belongs to (the newsgroup name).\n\nThis codelab walks you tuning a model with the API. [AI Studio](https://aistudio.google.com/app/tune) also supports creating new tuned models directly in the web UI, allowing you to quickly create and monitor models using data from Google Sheets, Drive or your own files.\n\n**Note**: We recommend doing this codelab first today. There may be a period of waiting while the model tunes, so if you start with this one, you can try the other codelab while you wait.","metadata":{"id":"4KDIFPAL2EnL"}},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"id":"9wafTyEH1_xF","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T16:45:54.386821Z","iopub.execute_input":"2026-02-10T16:45:54.387447Z","iopub.status.idle":"2026-02-10T16:46:14.059661Z","shell.execute_reply.started":"2026-02-10T16:45:54.387405Z","shell.execute_reply":"2026-02-10T16:46:14.057790Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"id":"T0CBG9xL2PvT","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T16:46:14.062484Z","iopub.execute_input":"2026-02-10T16:46:14.062928Z","iopub.status.idle":"2026-02-10T16:46:15.551213Z","shell.execute_reply.started":"2026-02-10T16:46:14.062877Z","shell.execute_reply":"2026-02-10T16:46:15.549428Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### Set up your API key\n\nTo run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n\nIf you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n\nTo make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.","metadata":{"id":"P4bYX2T72ScK"}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"id":"VuJPY3GK2SLZ","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T16:46:15.553094Z","iopub.execute_input":"2026-02-10T16:46:15.553996Z","iopub.status.idle":"2026-02-10T16:46:15.853298Z","shell.execute_reply.started":"2026-02-10T16:46:15.553940Z","shell.execute_reply":"2026-02-10T16:46:15.852316Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Explore available models\n\nYou will be using the [`TunedModel.create`](https://ai.google.dev/api/tuning#method:-tunedmodels.create) API method to start the fine-tuning job and create your custom model. Find a model that supports it through the [`models.list`](https://ai.google.dev/api/models#method:-models.list) endpoint. You can also find more information about tuning models in [the model tuning docs](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python).","metadata":{"id":"CqVA5QFO6n4z"}},{"cell_type":"code","source":"for model in client.models.list():\n    if \"createTunedModel\" in model.supported_actions:\n        print(model.name)","metadata":{"id":"coEacWAB6o0G","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T14:32:16.377816Z","iopub.execute_input":"2026-02-10T14:32:16.378632Z","iopub.status.idle":"2026-02-10T14:32:16.414699Z","shell.execute_reply.started":"2026-02-10T14:32:16.378589Z","shell.execute_reply":"2026-02-10T14:32:16.413728Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Download the dataset\n\nIn this activity, you will use the same newsgroups dataset that [you used to train a classifier in Keras](https://www.kaggle.com/code/markishere/day-2-classifying-embeddings-with-keras/). In this example you will use a fine-tuned Gemini model to achieve the same goal.\n\nThe [20 Newsgroups Text Dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) contains 18,000 newsgroups posts on 20 topics divided into training and test sets.","metadata":{"id":"peFm0w_0c1CO"}},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\n\nnewsgroups_train = fetch_20newsgroups(subset=\"train\")\nnewsgroups_test = fetch_20newsgroups(subset=\"test\")\n\n# View list of class names for dataset\nnewsgroups_train.target_names","metadata":{"id":"bX_kpgnQ9b-Z","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T16:46:15.855038Z","iopub.execute_input":"2026-02-10T16:46:15.855362Z","iopub.status.idle":"2026-02-10T16:46:27.404302Z","shell.execute_reply.started":"2026-02-10T16:46:15.855330Z","shell.execute_reply":"2026-02-10T16:46:27.402772Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"Here's what a single row looks like.","metadata":{"id":"ipafe6ptZFjt"}},{"cell_type":"code","source":"print(newsgroups_train.data[0])","metadata":{"id":"EtEXcdT39hCB","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T14:32:57.051497Z","iopub.execute_input":"2026-02-10T14:32:57.051926Z","iopub.status.idle":"2026-02-10T14:32:57.057483Z","shell.execute_reply.started":"2026-02-10T14:32:57.051896Z","shell.execute_reply":"2026-02-10T14:32:57.056256Z"}},"outputs":[{"name":"stdout","text":"From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Prepare the dataset\n\nYou'll use the same pre-processing code you used for the custom model on day 2. This pre-processing removes personal information, which can be used to \"shortcut\" to known users of a forum, and formats the text to appear a bit more like regular text and less like a newsgroup post (e.g. by removing the mail headers). This normalisation allows the model to generalise to regular text and not over-depend on specific fields. If your input data is always going to be newsgroup posts, it may be helpful to leave this structure in place if they provide genuine signals.","metadata":{"id":"03lDs1O4ZQ0-"}},{"cell_type":"code","source":"import email\nimport re\n\nimport pandas as pd\n\n\ndef preprocess_newsgroup_row(data):\n    # Extract only the subject and body\n    msg = email.message_from_string(data)\n    text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"\n    # Strip any remaining email addresses\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n    # Truncate the text to fit within the input limits\n    text = text[:40000]\n\n    return text\n\n\ndef preprocess_newsgroup_data(newsgroup_dataset):\n    # Put data points into dataframe\n    df = pd.DataFrame(\n        {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}\n    )\n    # Clean up the text\n    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n    # Match label to target name index\n    df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])\n\n    return df","metadata":{"id":"IoNYTxpoZgB0","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:07:03.538321Z","iopub.execute_input":"2026-02-10T17:07:03.539497Z","iopub.status.idle":"2026-02-10T17:07:03.548390Z","shell.execute_reply.started":"2026-02-10T17:07:03.539443Z","shell.execute_reply":"2026-02-10T17:07:03.546627Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Apply preprocessing to training and test datasets\ndf_train = preprocess_newsgroup_data(newsgroups_train)\ndf_test = preprocess_newsgroup_data(newsgroups_test)\n\ndf_train.head()","metadata":{"id":"kvOsUSRWaW4g","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:07:05.653382Z","iopub.execute_input":"2026-02-10T17:07:05.653994Z","iopub.status.idle":"2026-02-10T17:07:09.435567Z","shell.execute_reply.started":"2026-02-10T17:07:05.653932Z","shell.execute_reply":"2026-02-10T17:07:09.434359Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                                                Text  Label  \\\n0  WHAT car is this!?\\n\\n I was wondering if anyo...      7   \n1  SI Clock Poll - Final Call\\n\\nA fair number of...      4   \n2  PB questions...\\n\\nwell folks, my mac plus fin...      4   \n3  Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...      1   \n4  Re: Shuttle Launch Question\\n\\nFrom article <>...     14   \n\n              Class Name  \n0              rec.autos  \n1  comp.sys.mac.hardware  \n2  comp.sys.mac.hardware  \n3          comp.graphics  \n4              sci.space  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WHAT car is this!?\\n\\n I was wondering if anyo...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SI Clock Poll - Final Call\\n\\nA fair number of...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PB questions...\\n\\nwell folks, my mac plus fin...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Shuttle Launch Question\\n\\nFrom article &lt;&gt;...</td>\n      <td>14</td>\n      <td>sci.space</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"Now sample the data. You will keep 50 rows for each category for training. Note that this is even fewer than the Keras example, as this technique (parameter-efficient fine-tuning, or PEFT) updates a relatively small number of parameters and does not require training a new model or updating the large model.","metadata":{"id":"XSKcj5WtadaR"}},{"cell_type":"code","source":"def sample_data(df, num_samples, classes_to_keep):\n    # Sample rows, selecting num_samples of each Label.\n    df = (\n        df.groupby(\"Label\")[df.columns]\n        .apply(lambda x: x.sample(num_samples))\n        .reset_index(drop=True)\n    )\n\n    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n\n    return df\n\n\nTRAIN_NUM_SAMPLES = 50\nTEST_NUM_SAMPLES = 10\n# Keep rec.* and sci.*\nCLASSES_TO_KEEP = \"^rec|^sci\"\n\ndf_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\ndf_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)","metadata":{"id":"0t9Xu6X5akkt","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:07:09.437267Z","iopub.execute_input":"2026-02-10T17:07:09.437597Z","iopub.status.idle":"2026-02-10T17:07:09.472610Z","shell.execute_reply.started":"2026-02-10T17:07:09.437564Z","shell.execute_reply":"2026-02-10T17:07:09.471340Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Evaluate baseline performance\n\nBefore you start tuning a model, it's good practice to perform an evaluation on the available models to ensure you can measure how much the tuning helps.\n\nFirst identify a single sample row to use for visual inspection.","metadata":{}},{"cell_type":"code","source":"sample_idx = 0\nsample_row = preprocess_newsgroup_row(newsgroups_test.data[sample_idx])\nsample_label = newsgroups_test.target_names[newsgroups_test.target[sample_idx]]\n\nprint(sample_row)\nprint('---')\nprint('Label:', sample_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:07:14.425936Z","iopub.execute_input":"2026-02-10T17:07:14.426424Z","iopub.status.idle":"2026-02-10T17:07:14.433936Z","shell.execute_reply.started":"2026-02-10T17:07:14.426384Z","shell.execute_reply":"2026-02-10T17:07:14.432577Z"}},"outputs":[{"name":"stdout","text":"Need info on 88-89 Bonneville\n\n\n I am a little confused on all of the models of the 88-89 bonnevilles.\nI have heard of the LE SE LSE SSE SSEI. Could someone tell me the\ndifferences are far as features or performance. I am also curious to\nknow what the book value is for prefereably the 89 model. And how much\nless than book value can you usually get them for. In other words how\nmuch are they in demand this time of year. I have heard that the mid-spring\nearly summer is the best time to buy.\n\n\t\t\tNeil Gandler\n\n---\nLabel: rec.autos\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"Passing the text directly in as a prompt does not yield the desired results. The model will attempt to respond to the message.","metadata":{}},{"cell_type":"code","source":"response = client.models.generate_content(\n    model=\"gemini-2.0-flash\", contents=sample_row)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:25:01.261461Z","iopub.execute_input":"2026-02-10T17:25:01.262232Z","iopub.status.idle":"2026-02-10T17:25:14.620327Z","shell.execute_reply.started":"2026-02-10T17:25:01.262173Z","shell.execute_reply":"2026-02-10T17:25:14.618491Z"}},"outputs":[{"name":"stdout","text":"Okay, Neil, let's break down the 1988-89 Pontiac Bonneville models and address your valuation questions.\n\n**Understanding the 1988-89 Pontiac Bonneville Model Lineup**\n\nThese two years marked the beginning of the H-body Bonneville's second generation, a significant departure from its rear-wheel-drive predecessor. It was now a front-wheel-drive platform with a much more modern design. Here's a rundown of the trim levels, progressing from base to more luxurious/performance-oriented:\n\n*   **LE (Base Model):**\n    *   **Features:** The LE was the entry-level model. Standard features typically included:\n        *   Cloth upholstery\n        *   Air conditioning\n        *   AM/FM stereo\n        *   Basic instrumentation\n        *   Automatic transmission\n        *   Power windows and locks\n\n*   **SE (Sport Edition):**\n    *   **Features:** The SE was positioned as a step up from the LE, offering more features and a slightly sportier feel.\n        *   Upgraded cloth upholstery\n        *   Alloy wheels\n        *   Improved sound system\n        *   Often included options like cruise control and power seats\n\n*   **LSE (Luxury Sport Edition):**\n    *   **Features:** The LSE blended luxury features with a sporty character.\n        *   Leather seating was usually standard or optional.\n        *   More upscale trim and accents\n        *   Possible upgrades in suspension and handling (though not always)\n        *   Better sound systems and other convenience features\n\n*   **SSE (Sport Sedan Edition):**\n    *   **Features:** The SSE was the top-of-the-line, emphasizing both performance and luxury.\n        *   Standard leather seating.\n        *   Digital instrument cluster.\n        *   Full gauge package (oil pressure, voltage, etc.)\n        *   More aggressive styling elements (body cladding, spoilers)\n        *   Higher performance suspension tuning\n        *   Generally well-equipped with power options\n\n*   **SSEi (Supercharged Sport Edition):**\n    *   **Features:** The SSEi was the performance flagship of the Bonneville line.\n        *   Supercharged 3.8L V6 engine (a distinguishing feature)\n        *   All the features of the SSE, plus:\n        *   Specific body styling and badging to differentiate it\n        *   Performance-tuned suspension\n\n**Engine and Performance Differences**\n\n*   **LE, SE, LSE, SSE:** Typically came with the naturally aspirated 3.8L Buick V6 (3800 Series I). This engine was known for its reliability and decent power (around 165 horsepower).\n\n*   **SSEi:** Equipped with the supercharged 3.8L V6. This engine significantly boosted power output to around 205 horsepower and provided much stronger acceleration. The supercharged engine makes the SSEi the most desirable for performance enthusiasts.\n\n**Valuation and Market Demand**\n\nValuing a car as old as a 1989 Bonneville can be tricky because their worth is more tied to condition and mileage than a standard \"book value\". Here's a breakdown:\n\n*   **Book Value:** The traditional \"book value\" sources (like Kelley Blue Book or NADA) don't always accurately reflect the market for these cars, especially after 35+ years. They provide a *general* starting point, but you need to adjust based on condition and specific features.\n    *   Expect to see figures ranging from $1,000 to $4,000 depending on the source and the condition category they use. Realistically, the *market* value can be higher or lower.\n\n*   **Factors Affecting Market Value:**\n\n    *   **Condition:** This is *the* most important factor. A well-maintained Bonneville (especially an SSEi) in excellent condition will command a higher price. Look for:\n        *   Rust-free body and undercarriage\n        *   Clean interior\n        *   Well-maintained engine and transmission\n        *   No major mechanical issues\n        *   Original paint in good condition (or a quality repaint)\n    *   **Mileage:** Lower mileage is generally preferred, but a car with higher mileage that has been meticulously maintained might be a better buy than a low-mileage car that has been neglected.\n    *   **Model:** The SSEi models are the most sought-after due to their supercharged engine and performance enhancements. An SSE would be the next most desirable, followed by the LSE, SE, and LE.\n    *   **Options:** Features like leather seats, sunroof, digital dash (SSE/SSEi), and other desirable options can increase the value.\n    *   **Location:** Market demand can vary by region.  Areas where classic cars are more popular might see higher prices.\n    *   **Rarity:** Certain color combinations or unusual option packages can make a car more rare and potentially more valuable.\n    *   **Documentation:** Having original window stickers, maintenance records, and owner's manuals can increase the value, as it shows the car has been well cared for.\n\n*   **How Much Below Book Value Can You Get It?**\n\n    *   It depends *entirely* on the factors listed above.\n    *   In poor condition, you might get one for *well* below \"book value\" (or even scrap value).\n    *   In excellent condition (especially an SSEi), you might pay *more* than book value if it's a truly exceptional example.\n    *   Negotiate based on the specific car's condition and features. Point out any flaws or issues that need to be addressed.\n\n*   **Demand This Time of Year (Spring/Summer):**\n\n    *   Yes, generally, spring and early summer are often better times to buy classic cars. The weather is nicer for viewing cars, and people are more inclined to get out and enjoy them.\n    *   However, this also means there might be more competition from other buyers.\n    *   A good deal can be found at any time of year if you're patient and do your research.\n\n**Where to Look for Pricing Information**\n\n*   **Online Classifieds (eBay, Craigslist, Facebook Marketplace):** Search for similar Bonnevilles in your area (or nationally) to get a sense of current asking prices.  Pay close attention to the condition descriptions.\n*   **Classic Car Auctions (Bring a Trailer, Hemmings):** These can provide a more accurate picture of market values, especially for well-preserved or restored examples.  Look at past auction results for comparable cars.\n*   **Specialized Forums and Clubs:** Connect with Bonneville enthusiasts online. They can provide valuable insights into pricing and common issues to look for.\n\n**Tips for Buying a 1988-89 Bonneville:**\n\n1.  **Thorough Inspection:** Have a mechanic inspect the car before you buy it. Pay close attention to:\n    *   Engine and transmission health (especially the supercharger on SSEi models)\n    *   Rust (a common problem on cars of this age)\n    *   Suspension components\n    *   Brake system\n    *   Electrical system (especially the digital dash on SSE/SSEi models)\n2.  **Test Drive:** Drive the car extensively to check for any unusual noises, vibrations, or handling issues.\n3.  **Check for Modifications:** If the car has been modified, find out what was done and why. Modifications can affect reliability and value.\n4.  **Beware of Rust:** These cars are prone to rust, especially in areas with harsh winters. Inspect the body, undercarriage, and around the wheel wells carefully.\n5.  **Be Patient:** Don't rush into a purchase. Take your time to find a car that meets your needs and budget.\n\n**In summary:** Forget about relying heavily on \"book value.\" Assess each car individually based on its condition, mileage, features, and the desirability of the specific model (SSEi being the most sought-after). Do your research, be patient, and be prepared to negotiate. Good luck!\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"You can use the prompt engineering techniques you have learned this week to induce the model to perform the desired task. Try some of your own ideas and see what is effective, or check out the following cells for different approaches. Note that they have different levels of effectiveness!","metadata":{}},{"cell_type":"code","source":"# Ask the model directly in a zero-shot prompt.\n\nprompt = \"From what newsgroup does the following message originate?\"\nbaseline_response = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[prompt, sample_row])\nprint(baseline_response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:26:18.120723Z","iopub.execute_input":"2026-02-10T17:26:18.121309Z","iopub.status.idle":"2026-02-10T17:26:18.979143Z","shell.execute_reply.started":"2026-02-10T17:26:18.121262Z","shell.execute_reply":"2026-02-10T17:26:18.977573Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ask the model directly in a zero-shot prompt.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrom what newsgroup does the following message originate?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m baseline_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-2.0-flash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_row\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(baseline_response\u001b[38;5;241m.\u001b[39mtext)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/models.py:5370\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5368\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   5369\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5370\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5371\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m   5372\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5373\u001b[0m   logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   5374\u001b[0m   remaining_remote_calls_afc \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/models.py:4338\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4335\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[1;32m   4336\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[0;32m-> 4338\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[1;32m   4340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n\u001b[1;32m   4343\u001b[0m   response_dict \u001b[38;5;241m=\u001b[39m _GenerateContentResponse_from_vertex(\n\u001b[1;32m   4344\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client, response_dict\n\u001b[1;32m   4345\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/_api_client.py:640\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    632\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    636\u001b[0m ):\n\u001b[1;32m    637\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[1;32m    638\u001b[0m       http_method, path, request_dict, http_options\n\u001b[1;32m    639\u001b[0m   )\n\u001b[0;32m--> 640\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m   json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson\n\u001b[1;32m    642\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/_api_client.py:569\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    563\u001b[0m       method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    564\u001b[0m       url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m       timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    568\u001b[0m   )\n\u001b[0;32m--> 569\u001b[0m   \u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m    571\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m    572\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/errors.py:106\u001b[0m, in \u001b[0;36mAPIError.raise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    104\u001b[0m status_code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m--> 106\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response)\n","\u001b[0;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}"],"ename":"ClientError","evalue":"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}","output_type":"error"}],"execution_count":35},{"cell_type":"markdown","source":"This technique still produces quite a verbose response. You could try and parse out the relevant text, or refine the prompt even further.","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\n\n# You can use a system instruction to do more direct prompting, and get a\n# more succinct answer.\n\nsystem_instruct = \"\"\"\nYou are a classification service. You will be passed input that represents\na newsgroup post and you must respond with the newsgroup from which the post\noriginates.\n\"\"\"\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n# If you want to evaluate your own technique, replace this body of this function\n# with your model, prompt and other code and return the predicted answer.\n@retry.Retry(predicate=is_retriable)\ndef predict_label(post: str) -> str:\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=types.GenerateContentConfig(\n            system_instruction=system_instruct),\n        contents=post)\n\n    rc = response.candidates[0]\n\n    # Any errors, filters, recitation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        # Clean up the response.\n        return response.text.strip()\n\n\nprediction = predict_label(sample_row)\n\nprint(prediction)\nprint()\nprint(\"Correct!\" if prediction == sample_label else \"Incorrect.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:27:08.783692Z","iopub.execute_input":"2026-02-10T17:27:08.785317Z","iopub.status.idle":"2026-02-10T17:27:09.462142Z","shell.execute_reply.started":"2026-02-10T17:27:08.785259Z","shell.execute_reply":"2026-02-10T17:27:09.460838Z"}},"outputs":[{"name":"stdout","text":"rec.autos.makers.pontiac\n\nIncorrect.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"Now run a short evaluation using the function defined above. The test set is further sampled to ensure the experiment runs smoothly on the API's free tier. In practice you would evaluate over the whole set.","metadata":{}},{"cell_type":"code","source":"# Further sample the test data to be mindful of the free-tier quota.\ndf_baseline_eval = sample_data(df_test, 2, '.*')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:27:13.573128Z","iopub.execute_input":"2026-02-10T17:27:13.573582Z","iopub.status.idle":"2026-02-10T17:27:13.592142Z","shell.execute_reply.started":"2026-02-10T17:27:13.573548Z","shell.execute_reply":"2026-02-10T17:27:13.590706Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas.\ntqdmr.pandas()\n\n# But suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# # Further sample the test data to be mindful of the free-tier quota.\n# df_baseline_eval = sample_data(df_test, 2, '.*')\n\n# Make predictions using the sampled data.\ndf_baseline_eval['Prediction'] = df_baseline_eval['Text'].progress_apply(predict_label)\n\n# And calculate the accuracy.\naccuracy = (df_baseline_eval[\"Class Name\"] == df_baseline_eval[\"Prediction\"]).sum() / len(df_baseline_eval)\nprint(f\"Accuracy: {accuracy:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:30:35.514576Z","iopub.execute_input":"2026-02-10T17:30:35.515169Z","iopub.status.idle":"2026-02-10T17:32:29.680840Z","shell.execute_reply.started":"2026-02-10T17:30:35.515120Z","shell.execute_reply":"2026-02-10T17:32:29.678577Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e396dfe97b45ae8b95a16f3b4d65f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n","Cell \u001b[0;32mIn[36], line 19\u001b[0m, in \u001b[0;36mpredict_label\u001b[0;34m(post)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;129m@retry\u001b[39m\u001b[38;5;241m.\u001b[39mRetry(predicate\u001b[38;5;241m=\u001b[39mis_retriable)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_label\u001b[39m(post: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-2.0-flash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerateContentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43msystem_instruction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_instruct\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     rc \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/models.py:5370\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5369\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5370\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m   5372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5373\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/models.py:4338\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4336\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[0;32m-> 4338\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[1;32m   4340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/_api_client.py:640\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    637\u001b[0m http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[1;32m    638\u001b[0m     http_method, path, request_dict, http_options\n\u001b[1;32m    639\u001b[0m )\n\u001b[0;32m--> 640\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/_api_client.py:569\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    562\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    563\u001b[0m     method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    564\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    568\u001b[0m )\n\u001b[0;32m--> 569\u001b[0m \u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m    571\u001b[0m     response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m    572\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/errors.py:106\u001b[0m, in \u001b[0;36mAPIError.raise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m--> 106\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n","\u001b[0;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[39], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39mtqdm\u001b[38;5;241m.\u001b[39mTqdmExperimentalWarning)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# # Further sample the test data to be mindful of the free-tier quota.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# df_baseline_eval = sample_data(df_test, 2, '.*')\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make predictions using the sampled data.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m df_baseline_eval[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_baseline_eval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mText\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# And calculate the accuracy.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (df_baseline_eval[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass Name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m df_baseline_eval[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_baseline_eval)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n","File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/retry/retry_base.py:221\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m next_sleep \u001b[38;5;241m>\u001b[39m deadline:\n\u001b[1;32m    216\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    217\u001b[0m         error_list,\n\u001b[1;32m    218\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mTIMEOUT,\n\u001b[1;32m    219\u001b[0m         original_timeout,\n\u001b[1;32m    220\u001b[0m     )\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    222\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying due to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, sleeping \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124ms ...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(error_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], next_sleep)\n\u001b[1;32m    224\u001b[0m )\n","\u001b[0;31mRetryError\u001b[0m: Timeout of 120.0s exceeded, last exception: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}"],"ename":"RetryError","evalue":"Timeout of 120.0s exceeded, last exception: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}","output_type":"error"}],"execution_count":39},{"cell_type":"code","source":"## Adjustments due to quota issues - Batching\n\n# Adjusting the prompt to read batches\nsystem_instruct = \"\"\"\nYou are a classification service.\nYou will be passed a list of newsgroup posts.\nReturn one label per post, in the same order, one per line.\n\"\"\"\n\nis_retriable = lambda e: (\n    isinstance(e, genai.errors.APIError) and e.code in {429, 503}\n)\n\n# Adjusting the funtion to predict batches\n@retry.Retry(predicate=is_retriable)\ndef predict_batch(posts: list[str]) -> list[str]:\n    contents = \"\\n\\n\".join(\n        f\"Post {i+1}:\\n{post}\" for i, post in enumerate(posts)\n    )\n\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=types.GenerateContentConfig(\n            system_instruction=system_instruct\n        ),\n        contents=contents,\n    )\n\n    rc = response.candidates[0]\n\n    if rc.finish_reason.name != \"STOP\":\n        return [\"(error)\"] * len(posts)\n\n    return [line.strip() for line in response.text.splitlines()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:48:45.595523Z","iopub.execute_input":"2026-02-10T17:48:45.596012Z","iopub.status.idle":"2026-02-10T17:48:45.604079Z","shell.execute_reply.started":"2026-02-10T17:48:45.595973Z","shell.execute_reply":"2026-02-10T17:48:45.603049Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"## tqdmr may fail to show the last batch progress, but it works!\nBATCH_SIZE = 10\n\ntexts = df_baseline_eval[\"Text\"].tolist()\ntotal_rows = len(texts)\nprocessed_rows = 0\n\n## to make sure we got through all data, we'll define a funtion to show processed rows\ndef report_progress(processed_rows: int, total_rows: int) -> None:\n    pct = (processed_rows / total_rows) * 100\n    print(\n        f\"Processed {processed_rows}/{total_rows} rows \"\n        f\"({pct:.1f}%)\"\n    )\n\npredictions = []\n\nfor i in tqdmr(range(0, total_rows, BATCH_SIZE)):\n    batch = texts[i:i + BATCH_SIZE]\n    batch_preds = predict_batch(batch)\n    predictions.extend(batch_preds)\n\n    processed_rows += len(batch)\n    report_progress(processed_rows, total_rows)\n\ndf_baseline_eval[\"Prediction\"] = predictions\n\naccuracy = (\n    df_baseline_eval[\"Class Name\"]\n    == df_baseline_eval[\"Prediction\"]\n).mean()\n\nprint(f\"Accuracy: {accuracy:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:48:50.953477Z","iopub.execute_input":"2026-02-10T17:48:50.953969Z","iopub.status.idle":"2026-02-10T17:48:52.848160Z","shell.execute_reply.started":"2026-02-10T17:48:50.953927Z","shell.execute_reply":"2026-02-10T17:48:52.846945Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc85b11703b94cd6941bb6bdde155c19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed 10/16 rows (62.5%)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Processed 10/16 rows (62.5%)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed 16/16 rows (100.0%)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Processed 16/16 rows (100.0%)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Accuracy: 81.25%\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"## Tune a custom model\n\nIn this example you'll use tuning to create a model that requires no prompting or system instructions and outputs succinct text from the classes you provide in the training data.\n\nThe data contains both input text (the processed posts) and output text (the category, or newsgroup), that you can use to start tuning a model.\n\nWhen calling `tune()`, you can specify model tuning hyperparameters too:\n - `epoch_count`: defines how many times to loop through the data,\n - `batch_size`: defines how many rows to process in a single step, and\n - `learning_rate`: defines the scaling factor for updating model weights at each step.\n\nYou can also choose to omit them and use the defaults. [Learn more](https://developers.google.com/machine-learning/crash-course/linear-regression/hyperparameters) about these parameters and how they work. For this example these parameters were selected by running some tuning jobs and selecting parameters that converged efficiently.\n\nThis example will start a new tuning job, but only if one does not already exist. This allows you to leave this codelab and come back later - re-running this step will find your last model.","metadata":{"id":"Ok7ugrLzcghX"}},{"cell_type":"code","source":"for model in client.models.list():\n    if getattr(model, \"supports_tuning\", False):\n        print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:53:23.152656Z","iopub.execute_input":"2026-02-10T17:53:23.153781Z","iopub.status.idle":"2026-02-10T17:53:23.401940Z","shell.execute_reply.started":"2026-02-10T17:53:23.153669Z","shell.execute_reply":"2026-02-10T17:53:23.400670Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"## No tunable models found in the project...","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections.abc import Iterable\nimport random\n\n\n# Convert the data frame into a dataset suitable for tuning.\ninput_data = {'examples': \n    df_train[['Text', 'Class Name']]\n      .rename(columns={'Text': 'textInput', 'Class Name': 'output'})\n      .to_dict(orient='records')\n }\n\n# If you are re-running this lab, add your model_id here.\nmodel_id = None\n\n# Or try and find a recent tuning job.\nif not model_id:\n  queued_model = None\n  # Newest models first.\n  for m in reversed(client.tunings.list()):\n    # Only look at newsgroup classification models.\n    if m.name.startswith('tunedModels/newsgroup-classification-model'):\n      # If there is a completed model, use the first (newest) one.\n      if m.state.name == 'JOB_STATE_SUCCEEDED':\n        model_id = m.name\n        print('Found existing tuned model to reuse.')\n        break\n\n      elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n        # If there's a model still queued, remember the most recent one.\n        queued_model = m.name\n  else:\n    if queued_model:\n      model_id = queued_model\n      print('Found queued model, still waiting.')\n\n\n# Upload the training data and queue the tuning job.\nif not model_id:\n    tuning_op = client.tunings.tune(\n        base_model=\"models/gemini-1.5-flash-001-tuning\",\n        training_dataset=input_data,\n        config=types.CreateTuningJobConfig(\n            tuned_model_display_name=\"Newsgroup classification model\",\n            batch_size=16,\n            epoch_count=2,\n        ),\n    )\n\n    print(tuning_op.state)\n    model_id = tuning_op.name\n\nprint(model_id)","metadata":{"id":"pWOZlspfY8dV","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:54:40.228346Z","iopub.execute_input":"2026-02-10T17:54:40.228838Z","iopub.status.idle":"2026-02-10T17:54:40.693782Z","shell.execute_reply.started":"2026-02-10T17:54:40.228796Z","shell.execute_reply":"2026-02-10T17:54:40.691891Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Upload the training data and queue the tuning job.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_id:\n\u001b[0;32m---> 39\u001b[0m     tuning_op \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtunings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/gemini-1.5-flash-001-tuning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateTuningJobConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtuned_model_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNewsgroup classification model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepoch_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tuning_op\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m     50\u001b[0m     model_id \u001b[38;5;241m=\u001b[39m tuning_op\u001b[38;5;241m.\u001b[39mname\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/_common.py:312\u001b[0m, in \u001b[0;36mexperimental_warning.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m   warning_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    307\u001b[0m   warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    308\u001b[0m       message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m    309\u001b[0m       category\u001b[38;5;241m=\u001b[39merrors\u001b[38;5;241m.\u001b[39mExperimentalWarning,\n\u001b[1;32m    310\u001b[0m       stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    311\u001b[0m   )\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/tunings.py:1031\u001b[0m, in \u001b[0;36mTunings.tune\u001b[0;34m(self, base_model, training_dataset, config)\u001b[0m\n\u001b[1;32m   1025\u001b[0m   tuning_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tune(\n\u001b[1;32m   1026\u001b[0m       base_model\u001b[38;5;241m=\u001b[39mbase_model,\n\u001b[1;32m   1027\u001b[0m       training_dataset\u001b[38;5;241m=\u001b[39mtraining_dataset,\n\u001b[1;32m   1028\u001b[0m       config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m   1029\u001b[0m   )\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1031\u001b[0m   operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tune_mldev\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m operation\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtunedModel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m operation\u001b[38;5;241m.\u001b[39mmetadata:\n\u001b[1;32m   1037\u001b[0m     tuned_model_name \u001b[38;5;241m=\u001b[39m operation\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtunedModel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/tunings.py:970\u001b[0m, in \u001b[0;36mTunings._tune_mldev\u001b[0;34m(self, base_model, training_dataset, config)\u001b[0m\n\u001b[1;32m    967\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[1;32m    968\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[0;32m--> 970\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n\u001b[1;32m    975\u001b[0m   response_dict \u001b[38;5;241m=\u001b[39m _Operation_from_vertex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client, response_dict)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/_api_client.py:640\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    632\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    636\u001b[0m ):\n\u001b[1;32m    637\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[1;32m    638\u001b[0m       http_method, path, request_dict, http_options\n\u001b[1;32m    639\u001b[0m   )\n\u001b[0;32m--> 640\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m   json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson\n\u001b[1;32m    642\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/_api_client.py:569\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    563\u001b[0m       method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    564\u001b[0m       url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m       timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    568\u001b[0m   )\n\u001b[0;32m--> 569\u001b[0m   \u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m    571\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m    572\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/errors.py:106\u001b[0m, in \u001b[0;36mAPIError.raise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    104\u001b[0m status_code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m--> 106\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response)\n","\u001b[0;31mClientError\u001b[0m: 400 FAILED_PRECONDITION. {'error': {'code': 400, 'message': 'models/gemini-1.5-flash-001-tuning is not found for CREATE TUNED MODEL at API version v1beta.', 'status': 'FAILED_PRECONDITION'}}"],"ename":"ClientError","evalue":"400 FAILED_PRECONDITION. {'error': {'code': 400, 'message': 'models/gemini-1.5-flash-001-tuning is not found for CREATE TUNED MODEL at API version v1beta.', 'status': 'FAILED_PRECONDITION'}}","output_type":"error"}],"execution_count":47},{"cell_type":"markdown","source":"This has created a tuning job that will run in the background. To inspect the progress of the tuning job, run this cell to plot the current status and loss curve. Once the status reaches `ACTIVE`, tuning is complete and the model is ready to use.\n\nTuning jobs are queued, so it may look like no training steps have been taken initially but it will progress. Tuning can take anywhere from a few minutes to multiple hours, depending on factors like your dataset size and how busy the tuning infrastrature is. Why not treat yourself to a nice cup of tea while you wait, or come and say \"Hi!\" in the group [Discord](https://discord.com/invite/kaggle).\n\nIt is safe to stop this cell at any point. It will not stop the tuning job.\n\n**IMPORTANT**: Due to the high volume of users doing this course, tuning jobs may be queued for many hours. Take a note of your tuned model ID above (`tunedModels/...`) so you can come back to it tomorrow. In the meantime, check out the [Search grounding](https://www.kaggle.com/code/markishere/day-4-google-search-grounding/) codelab. If you want to try tuning a local LLM, check out [the fine-tuning guides for tuning a Gemma model](https://ai.google.dev/gemma/docs/tune).","metadata":{"id":"NQ3YZ2MBubCY"}},{"cell_type":"code","source":"import datetime\nimport time\n\n\nMAX_WAIT = datetime.timedelta(minutes=10)\n\nwhile not (tuned_model := client.tunings.get(name=model_id)).has_ended:\n\n    print(tuned_model.state)\n    time.sleep(60)\n\n    # Don't wait too long. Use a public model if this is going to take a while.\n    if datetime.datetime.now(datetime.timezone.utc) - tuned_model.create_time > MAX_WAIT:\n        print(\"Taking a shortcut, using a previously prepared model.\")\n        model_id = \"tunedModels/newsgroup-classification-model-ltenbi1b\"\n        tuned_model = client.tunings.get(name=model_id)\n        break\n\n\nprint(f\"Done! The model state is: {tuned_model.state.name}\")\n\nif not tuned_model.has_succeeded and tuned_model.error:\n    print(\"Error:\", tuned_model.error)","metadata":{"id":"c4ef5f13692d","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:11:13.878313Z","iopub.execute_input":"2025-04-02T08:11:13.878725Z","iopub.status.idle":"2025-04-02T08:11:15.417549Z","shell.execute_reply.started":"2025-04-02T08:11:13.878693Z","shell.execute_reply":"2025-04-02T08:11:15.416441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Use the new model\n\nNow that you have a tuned model, try it out with custom data. You use the same API as a normal Gemini API interaction, but you specify your new model as the model name, which will start with `tunedModels/`.","metadata":{"id":"9-qiIdK4u80z"}},{"cell_type":"code","source":"new_text = \"\"\"\nFirst-timer looking to get out of here.\n\nHi, I'm writing about my interest in travelling to the outer limits!\n\nWhat kind of craft can I buy? What is easiest to access from this 3rd rock?\n\nLet me know how to do that please.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model=model_id, contents=new_text)\n\nprint(response.text)","metadata":{"id":"hyO2-MXLvM6a","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:11:24.915979Z","iopub.execute_input":"2025-04-02T08:11:24.91659Z","iopub.status.idle":"2025-04-02T08:11:27.566345Z","shell.execute_reply.started":"2025-04-02T08:11:24.91654Z","shell.execute_reply":"2025-04-02T08:11:27.564861Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation\n\nYou can see that the model outputs labels that correspond to those in the training data, and without any system instructions or prompting, which is already a great improvement. Now see how well it performs on the test set.\n\nNote that there is no parallelism in this example; classifying the test sub-set will take a few minutes.","metadata":{"id":"xajLek9DySH_"}},{"cell_type":"code","source":"@retry.Retry(predicate=is_retriable)\ndef classify_text(text: str) -> str:\n    \"\"\"Classify the provided text into a known newsgroup.\"\"\"\n    response = client.models.generate_content(\n        model=model_id, contents=text)\n    rc = response.candidates[0]\n\n    # Any errors, filters, recitation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        return rc.content.parts[0].text\n\n\n# The sampling here is just to minimise your quota usage. If you can, you should\n# evaluate the whole test set with `df_model_eval = df_test.copy()`.\ndf_model_eval = sample_data(df_test, 4, '.*')\n\ndf_model_eval[\"Prediction\"] = df_model_eval[\"Text\"].progress_apply(classify_text)\n\naccuracy = (df_model_eval[\"Class Name\"] == df_model_eval[\"Prediction\"]).sum() / len(df_model_eval)\nprint(f\"Accuracy: {accuracy:.2%}\")","metadata":{"id":"6T2Y3ZApvbMw","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:11:29.712728Z","iopub.execute_input":"2025-04-02T08:11:29.713129Z","iopub.status.idle":"2025-04-02T08:14:02.348228Z","shell.execute_reply.started":"2025-04-02T08:11:29.713082Z","shell.execute_reply":"2025-04-02T08:14:02.347128Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compare token usage\n\nAI Studio and the Gemini API provide model tuning at no cost, however normal limits and charges apply for *use* of a tuned model.\n\nThe size of the input prompt and other generation config like system instructions, as well as the number of generated output tokens, all contribute to the overall cost of a request.","metadata":{}},{"cell_type":"code","source":"# Calculate the *input* cost of the baseline model with system instructions.\nsysint_tokens = client.models.count_tokens(\n    model='gemini-1.5-flash-001', contents=[system_instruct, sample_row]\n).total_tokens\nprint(f'System instructed baseline model: {sysint_tokens} (input)')\n\n# Calculate the input cost of the tuned model.\ntuned_tokens = client.models.count_tokens(model=tuned_model.base_model, contents=sample_row).total_tokens\nprint(f'Tuned model: {tuned_tokens} (input)')\n\nsavings = (sysint_tokens - tuned_tokens) / tuned_tokens\nprint(f'Token savings: {savings:.2%}')  # Note that this is only n=1.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:14:02.35288Z","iopub.execute_input":"2025-04-02T08:14:02.353306Z","iopub.status.idle":"2025-04-02T08:14:02.612742Z","shell.execute_reply.started":"2025-04-02T08:14:02.353257Z","shell.execute_reply":"2025-04-02T08:14:02.611517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The earlier verbose model also produced more output tokens than needed for this task.","metadata":{}},{"cell_type":"code","source":"baseline_token_output = baseline_response.usage_metadata.candidates_token_count\nprint('Baseline (verbose) output tokens:', baseline_token_output)\n\ntuned_model_output = client.models.generate_content(\n    model=model_id, contents=sample_row)\ntuned_tokens_output = tuned_model_output.usage_metadata.candidates_token_count\nprint('Tuned output tokens:', tuned_tokens_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:14:02.614741Z","iopub.execute_input":"2025-04-02T08:14:02.615096Z","iopub.status.idle":"2025-04-02T08:14:03.532486Z","shell.execute_reply.started":"2025-04-02T08:14:02.615061Z","shell.execute_reply":"2025-04-02T08:14:03.531188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Next steps\n\nNow that you have tuned a classification model, try some other tasks, like tuning a model to respond with a specific tone or style using hand-written examples (or even generated examples!). Kaggle hosts [a number of datasets](https://www.kaggle.com/datasets) you can try out.\n\nLearn about [when supervised fine-tuning is most effective](https://cloud.google.com/blog/products/ai-machine-learning/supervised-fine-tuning-for-gemini-llm).\n\nAnd check out the [fine-tuning tutorial](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?hl=en&lang=python) for another example that shows a tuned model extending beyond the training data to new, unseen inputs.\n\n*- [Mark McD](https://linktr.ee/markmcd)*","metadata":{"id":"6c1204a5d0ab"}}]}